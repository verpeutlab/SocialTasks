# -*- coding: utf-8 -*-
"""Top-down SLEAP Three-Chamber Upload

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lGwm1zzkSwb0zG9mNIeNdlSDZEWEs5v9

This notebook shows you how to run training and inference on your own SLEAP dataset using the command-line interface.

You'll use [Google Drive](https://www.google.com/drive) for moving your data to and from Colab. We'll guide you through the process.

# Installation

Let's use `pip` to install SLEAP from PyPI.

Note: This installation method should also work on other Linux machines, such as an HPC cluster, or on any system where you aren't planning to use a GPU. To use a GPU on a Windows machine you'll need to install using `conda`.
"""

!pip uninstall -y opencv-python opencv-contrib-python
!pip install sleap==1.3.1

"""## Getting your training data into Colab

You'll need to get your training data into Colab. The easiest way to do this is to export a self-contained **training package** from SLEAP and copy that on to [Google Drive](https://www.google.com/drive).

Let's first mount your Google Drive so that it can be accessed from Colab. You'll be prompted to log into your Google account, give Colab access, and the copy an authorization code into a field below.
"""

from google.colab import drive
drive.mount('/content/drive/')

"""Now let's create a training package and copy it into your Google Drive.

A training package contains both labeled data as well as the labeled images which will be used for training. One advantage to training packages is that it doesn't depend on paths to other files (i.e., videos) to be messed up when you copy your project to another volume.

See [this guide](https://sleap.ai/guides/training-package.html) for exporting a training package from SLEAP.



The following cell sets your current working directory to the directory with your training package. This will ensure that the output from training (i.e., the models) and from interence (i.e., predictions) will all be saved in this directory.

**Important**: For this demo, I'll assume you have a `sleap` directory in the root of your Google drive and you place the exported training package in that directory. The name of the package will need to be inserted wherever the code says file.
"""

import os
os.chdir('/content/drive/My Drive/ProjectThreeChamberModel') #create an empty folder where you will place saved model into

from google.colab import drive
drive.mount('/content/drive')

"""
At this point you should have SLEAP installed, your Google Drive mounted, and trained models saved on your Google Drive. If you've been working through the notebook, you should have a `models` subdirectory inside your current working directory. Let's take a look:"""

!ls models

"""# Model evaluation

After you've trained several models, you may want to generate some accuracy metrics to compare between them. This notebook demonstrates how you'll do that given a trained model.

Let's start by installing SLEAP and downloading the trained model.

**Important:** This analysis is currently only compatible with centered_instance models
"""

!apt install tree

"""A trained SLEAP model will be a folder containing files that specify metadata that is useful for evaluation and analysis. The exact set of files may depend on the configuration, but all models will come with:

- `metrics.train.npz`: Metrics for the training split.
- `metrics.val.npz`: Metrics for the validation split. This is what you'll want to use most of the time since it wasn't directly used for optimizing the model.

**Note:** A test split will also be evaluated if it was provided during training and saved to `metrics.test.npz`.
"""

!tree models/240308_170001.single_instance.n=760 #240303_164358.centered_instance.n=760

"""Additionally, the following files are included and may also be useful:
- `best_model.h5`: The actual saved model and weights. This can be loaded with `tf.keras.model.load_model()` but it is recommended to use `sleap.load_model()` instead as it takes care of adding some additional inference-only procedures.
- `training_config.json`: The configuration for the model training job, including metadata inferred during the training procedure. It can be loaded with `sleap.load_config()`.
- `labels_gt.train.slp` and `labels_pr.train.slp`: These are SLEAP labels files containing the ground truth and predicted points for the training split. They do not contain the images, but can be used to retrieve the poses used.
- `labels_gt.train.slp` and `labels_pr.train.slp`: These are SLEAP labels files containing the ground truth and predicted points for the validation split. They do not contain the images, but can be used to retrieve the poses used.
"""

import sleap
import numpy as np
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

mpl.style.use("seaborn-deep")
sleap.versions()

"""SLEAP metrics can be loaded using the `sleap.load_metrics()` API:"""



"""Loading the metrics for the validation split of the model we can see all of the available keys:"""

metrics = sleap.load_metrics("models/240308_170001.single_instance.n=760", split="val") #240303_164358.centered_instance.n=760
print("\n".join(metrics.keys()))

"""To start, let's look at the summary of the **localization errors**:"""

print("Error distance (50%):", metrics["dist.p50"])
print("Error distance (90%):", metrics["dist.p90"])
print("Error distance (95%):", metrics["dist.p95"])

"""These are the percentiles of the distribution of how far off the model was from the ground truth location.

We can visualize the entire distribution like this:
"""

plt.figure(figsize=(6, 3), dpi=150, facecolor="w")
sns.histplot(metrics["dist.dists"].flatten(), binrange=(0, 20), kde=True, kde_kws={"clip": (0, 20)}, stat="probability")
plt.xlabel("Localization error (px)");

"""This metric is intuitive, but it does not incorporate other sources of error like those stemming from poor instance detection and grouping, or missing points.

The Object Keypoint Similarity (OKS) is a more holistic metric that takes factors such as landmark visibility, animal size, and the difficulty in locating keypoints (all are assumed to be "easy" for our calculations). You can read more about this and other pose estimation metrics in: https://arxiv.org/abs/1707.05388

First let's plot the distribution of OKS scores:
"""

plt.figure(figsize=(6, 3), dpi=150, facecolor="w")
sns.histplot(metrics["oks_voc.match_scores"].flatten(), binrange=(0, 1), kde=True, kde_kws={"clip": (0, 1)}, stat="probability")
plt.xlabel("Object Keypoint Similarity");

"""Since these range from 0 to 1, it seems like we're doing pretty well!

Another way to summarize this is through precision-recall curves, which evaluate how well the model does at different thresholds of OKS scores. The higher the threshold, the more stringent our criteria for classifying a prediction as correct.

Here we plot this at different thresholds:
"""

plt.figure(figsize=(4, 4), dpi=150, facecolor="w")
for precision, thresh in zip(metrics["oks_voc.precisions"][::2], metrics["oks_voc.match_score_thresholds"][::2]):
    plt.plot(metrics["oks_voc.recall_thresholds"], precision, "-", label=f"OKS @ {thresh:.2f}")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.legend(loc="lower left");

"""An easy way to summarize this analysis is to take the average over all of these thresholds to compute the **mean Average Precision (mAP)** and **mean Average Recall (mAR)** which are widely used in the pose estimation literature.

Here are those values saved out:
"""

print("mAP:", metrics["oks_voc.mAP"])
print("mAR:", metrics["oks_voc.mAR"])

"""Now we can run some of the same visualizations for the centroid model. (Note: the first two graphs canot be generated.)"""

!tree models/230208_083239.centroid

metrics = sleap.load_metrics("models/230208_083239.centroid", split="val")
print("\n".join(metrics.keys()))

print("Error distance (50%):", metrics["dist.p50"])
print("Error distance (90%):", metrics["dist.p90"])
print("Error distance (95%):", metrics["dist.p95"])

plt.figure(figsize=(4, 4), dpi=150, facecolor="w")
for precision, thresh in zip(metrics["oks_voc.precisions"][::2], metrics["oks_voc.match_score_thresholds"][::2]):
    plt.plot(metrics["oks_voc.recall_thresholds"], precision, "-", label=f"OKS @ {thresh:.2f}")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.legend(loc="lower left");

print("mAP:", metrics["oks_voc.mAP"])
print("mAR:", metrics["oks_voc.mAR"])