# -*- coding: utf-8 -*-
"""SLEAP Open Field Model Training and Evaluation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SJ9Kp7KmUT-as-Qr1M27_l0KNm5oAtrx

This notebook shows you how to run training and inference on your own SLEAP dataset using the command-line interface.

You'll use [Google Drive](https://www.google.com/drive) for moving your data to and from Colab. We'll guide you through the process.

# Installation

Let's use `pip` to install SLEAP from PyPI.

Note: This installation method should also work on other Linux machines, such as an HPC cluster, or on any system where you aren't planning to use a GPU. To use a GPU on a Windows machine you'll need to install using `conda`.
"""

!pip uninstall -y opencv-python opencv-contrib-python
!pip install sleap==1.3.1

"""## Getting your training data into Colab

You'll need to get your training data into Colab. The easiest way to do this is to export a self-contained **training package** from SLEAP and copy that on to [Google Drive](https://www.google.com/drive).

Let's first mount your Google Drive so that it can be accessed from Colab. You'll be prompted to log into your Google account, give Colab access, and the copy an authorization code into a field below.
"""

from google.colab import drive
drive.mount('/content/drive/')

"""Now let's create a training package and copy it into your Google Drive.

A training package contains both labeled data as well as the labeled images which will be used for training. One advantage to training packages is that it doesn't depend on paths to other files (i.e., videos) to be messed up when you copy your project to another volume.

See [this guide](https://sleap.ai/guides/training-package.html) for exporting a training package from SLEAP.



The following cell sets your current working directory to the directory with your training package. This will ensure that the output from training (i.e., the models) and from interence (i.e., predictions) will all be saved in this directory.

**Important**: For this demo, I'll assume you have a `sleap` directory in the root of your Google drive and you place the exported training package in that directory. The name of the package will need to be inserted wherever the code says file.
"""

from google.colab import drive
drive.mount('/content/drive')

import os
os.chdir('/content/drive/My Drive/two_mouse_training_12_23_test_C249LR.pkg copy.slp.training_job') #create an empty folder where all files needed and generated from this process will be stored

"""## Training

Now you're ready to train a model! We'll use the command-line interface for training, and train a model for confidence maps using the default **training profile**. The training profile determines the model architecture, the learning rate for training, and other training hyperparameters.

When you start running this cell, you'll see the training parameters listed and then you'll see the training and validation loss for each training epoch.

If you're happy with the validation loss you see for an epoch during training, you're welcome to stop training by clicking the stop button next to the notebook cell running training. The version of the model with the lowest validation loss is saved during training, and that's what will be used for inference. If you don't stop training, it will run for 200 epochs, or until validation loss fails to improve for some number of epochs (controlled by the `early_stopping` parameter in the training profile).

**New:** The tensorboard extension below will show you live updates on how your training is progressing. Run this cell before you start training, but be aware that it will initially tell you that there are no active datasets. You will just need to hit the refresh button in the top right corner.
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir "/content/drive/My Drive/two_mouse_training_12_23_test_C249LR.pkg copy.slp.training_job" --reload_multifile=true --reload_interval=1 --max_reload_threads=2

"""Once training finishes, you'll have a trained model for confidence maps on your Google Drive. There will be a `models/` directory inside your `sleap/` directory (or wherever you had the training package), and inside this there will be a directory for each model.

These directories contain all the files SLEAP needs to use this model. You can copy it to a local drive if you want to use it for running inference from the SLEAP GUI, copy it to a network drive if you want to run inference from an HPC cluster, or leave it on your Google Drive if you want to run inference on Colab... as we'll do below.

**Important**: Adjust the file name to that of your training package. You will also need to copy the centered_instance.json **and the centroid.json** files into the directory that you are using. They are currently located in the lab drive, in the folder "Colab for SLEAP".
"""

!sleap-train centered_instance.json two_mouse_training_12_20_test_C249LR.pkg.slp

!sleap-train centroid.json two_mouse_training_12_20_test_C249LR.pkg.slp #--run_name "models/centroid"

"""
At this point you should have SLEAP installed, your Google Drive mounted, and trained models saved on your Google Drive. If you've been working through the notebook, you should have a `models` subdirectory inside your current working directory. Let's take a look:"""

!ls models

"""# Model evaluation

After you've trained several models, you may want to generate some accuracy metrics to compare between them. This notebook demonstrates how you'll do that given a trained model.

Let's start by installing SLEAP and downloading the trained model.

**Important:** This analysis is currently only compatible with centered_instance models
"""

!apt install tree

"""A trained SLEAP model will be a folder containing files that specify metadata that is useful for evaluation and analysis. The exact set of files may depend on the configuration, but all models will come with:

- `metrics.train.npz`: Metrics for the training split.
- `metrics.val.npz`: Metrics for the validation split. This is what you'll want to use most of the time since it wasn't directly used for optimizing the model.

**Note:** A test split will also be evaluated if it was provided during training and saved to `metrics.test.npz`.
"""

!tree models/221222_090406.centered_instance

"""Additionally, the following files are included and may also be useful:
- `best_model.h5`: The actual saved model and weights. This can be loaded with `tf.keras.model.load_model()` but it is recommended to use `sleap.load_model()` instead as it takes care of adding some additional inference-only procedures.
- `training_config.json`: The configuration for the model training job, including metadata inferred during the training procedure. It can be loaded with `sleap.load_config()`.
- `labels_gt.train.slp` and `labels_pr.train.slp`: These are SLEAP labels files containing the ground truth and predicted points for the training split. They do not contain the images, but can be used to retrieve the poses used.
- `labels_gt.train.slp` and `labels_pr.train.slp`: These are SLEAP labels files containing the ground truth and predicted points for the validation split. They do not contain the images, but can be used to retrieve the poses used.
"""

import sleap
import numpy as np
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

mpl.style.use("seaborn-deep")
sleap.versions()

"""SLEAP metrics can be loaded using the `sleap.load_metrics()` API:

Loading the metrics for the validation split of the model we can see all of the available keys:
"""

metrics = sleap.load_metrics("models/221222_090406.centered_instance", split="val")
print("\n".join(metrics.keys()))

"""To start, let's look at the summary of the **localization errors**:"""

print("Error distance (50%):", metrics["dist.p50"])
print("Error distance (90%):", metrics["dist.p90"])
print("Error distance (95%):", metrics["dist.p95"])

"""These are the percentiles of the distribution of how far off the model was from the ground truth location.

We can visualize the entire distribution like this:
"""

plt.figure(figsize=(6, 3), dpi=150, facecolor="w")
sns.histplot(metrics["dist.dists"].flatten(), binrange=(0, 20), kde=True, kde_kws={"clip": (0, 20)}, stat="probability")
plt.xlabel("Localization error (px)");

"""This metric is intuitive, but it does not incorporate other sources of error like those stemming from poor instance detection and grouping, or missing points.

The Object Keypoint Similarity (OKS) is a more holistic metric that takes factors such as landmark visibility, animal size, and the difficulty in locating keypoints (all are assumed to be "easy" for our calculations). You can read more about this and other pose estimation metrics in: https://arxiv.org/abs/1707.05388

First let's plot the distribution of OKS scores:
"""

plt.figure(figsize=(6, 3), dpi=150, facecolor="w")
sns.histplot(metrics["oks_voc.match_scores"].flatten(), binrange=(0, 1), kde=True, kde_kws={"clip": (0, 1)}, stat="probability")
plt.xlabel("Object Keypoint Similarity");

"""Since these range from 0 to 1, it seems like we're doing pretty well!

Another way to summarize this is through precision-recall curves, which evaluate how well the model does at different thresholds of OKS scores. The higher the threshold, the more stringent our criteria for classifying a prediction as correct.

Here we plot this at different thresholds:
"""

plt.figure(figsize=(4, 4), dpi=150, facecolor="w")
for precision, thresh in zip(metrics["oks_voc.precisions"][::2], metrics["oks_voc.match_score_thresholds"][::2]):
    plt.plot(metrics["oks_voc.recall_thresholds"], precision, "-", label=f"OKS @ {thresh:.2f}")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.legend(loc="lower left");

"""An easy way to summarize this analysis is to take the average over all of these thresholds to compute the **mean Average Precision (mAP)** and **mean Average Recall (mAR)** which are widely used in the pose estimation literature.

Here are those values saved out:
"""

print("mAP:", metrics["oks_voc.mAP"])
print("mAR:", metrics["oks_voc.mAR"])

"""Now we can run some of the same visualizations for the centroid model. (Note: the first two graphs canot be generated.)"""

!tree models/221222_090406.centroid

metrics = sleap.load_metrics("models/221222_090406.centroid", split="val")
print("\n".join(metrics.keys()))

print("Error distance (50%):", metrics["dist.p50"])
print("Error distance (90%):", metrics["dist.p90"])
print("Error distance (95%):", metrics["dist.p95"])

plt.figure(figsize=(4, 4), dpi=150, facecolor="w")
for precision, thresh in zip(metrics["oks_voc.precisions"][::2], metrics["oks_voc.match_score_thresholds"][::2]):
    plt.plot(metrics["oks_voc.recall_thresholds"], precision, "-", label=f"OKS @ {thresh:.2f}")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.legend(loc="lower left");

print("mAP:", metrics["oks_voc.mAP"])
print("mAR:", metrics["oks_voc.mAR"])

"""# Inference
Once we're satisfied with our model, we'll need a video for which we want predictions. Copy the video onto your Google drive and change the file name below to the name of your video.

You can now start tracking. You can change the number of frames to be tracked, or run on the whole video by removing that line. If you are tracking multiple animals, alter the instance_count number.
"""

!sleap-track OF_C349NSaline-10252023092328.avi \ #/Users/megan_nelson/Desktop/CURRENT_DESKTOP/Verpeut_Lab/Two_Animal_Open_Field_Training/10_6_two_animal_test-10062022113100.avi \ --frames 0-1000 \
  --n_instances 2 \
  --n_nodes 13 \
  --tracking.min_points 13 \
  --tracking.TrackCleaner.instance_count 2\
  --tracking.tracker simple \
   --tracking.clean_instance_count 2\
  --tracking.target_instance_count 2\
  --tracking.pre_cull_to_target 2\
  --tracking.min_new_track_points 13\
  --tracking.post_connect_single_breaks 2\
  -m models/221222_090406.centered_instance \
  -m models/221222_090406.centroid \
  -o OF_C349NSaline-10252023092328.avi_1_22.avi.predictions.slp
  #-o two_animal_12_13.predictions.slp

"""When inference is finished, it will save the predictions in a file which can be opened in the GUI as a SLEAP project file. The file will be in the same directory as the video and the filename will be `{video filename}.predictions.slp`.

Let's inspect the predictions file:
"""

video = VideoMarkerThread.save_labeled_video('OF_C257R-12062022102746.avi',OF_C257R-12062022102746.avi_sigma15nin_1to10000_12_22.avi.predictions.slp,OF_C257R-12062022102746.avi)

!sleap-inspect OF_C249LR-10202022091104.avi.predictions.slp #two_animal_12_12.predictions.h5.slp #OF_C249LR.predictions.slp

"""You can copy this file from your Google Drive to a local drive and open it in the SLEAP GUI app (or open it directly if you have your Google Drive mounted on your local machine). If the video is in the same directory as the predictions file, SLEAP will automatically find it; otherwise, you'll be prompted to locate the video (since the path to the video on your local machine will be different than the path to the video on Colab).

## Parameters

One important option when running inference is whether (and how) you want to track instance identities. If you omit `--tracking.tracker flow` then the identities will not be tracked. Tracking methods/options are explained [here](https://sleap.ai/guides/proofreading.html#tracking-methods).

You can see all of the command-line arguments for both analysis and tracking here:
"""

help(sleap.load_metrics)

!sleap-track --help